{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934f15e9",
   "metadata": {},
   "source": [
    "# Verb prediction (bidirectional ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d00611",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giuliarambelli/Event_Knowledge_ANN/blob/master/ANNs_predict-verb-masked.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-middle",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function load the datasets modified, the 3rd column is the position of the word we have to mask\n",
    "def load_data(inpath):\n",
    "    idxs = []\n",
    "    sentences = []\n",
    "    pos = []\n",
    "    with open(inpath, 'r') as f:\n",
    "        for line in f:\n",
    "            idx, sentence, target_pos = line.strip().split('\\t')\n",
    "            idxs.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            pos.append(int(target_pos))\n",
    "    return idxs, sentences, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset new-EventsAdapt-sentences.ids.txt\n",
    "idxs_sent, sentences, pos = load_data('originals/new-EventsAdapt-sentences.ids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f417482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function load for word2word mask tasks\n",
    "def prepare_data(df):\n",
    "    ids = []\n",
    "    sents = []\n",
    "    for index, row in df.iterrows():\n",
    "        ids.append(row[0])\n",
    "        if row[1][-1]!='.':\n",
    "            sents.append(row[1]+' .')\n",
    "        else:\n",
    "            sents.append(row[1])\n",
    "    return (ids, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load just ids and sentences (for word-by-word preds)\n",
    "dtfit=pd.read_csv('originals/DTFit_vassallo_deps.txt', sep='\\t', header=None)\n",
    "ev1=pd.read_csv('originals/ev1_deps.txt', sep='\\t',header=None)\n",
    "ev2=pd.read_csv('originals/ev2_deps.txt', sep='\\t',header=None)\n",
    "new_ev=pd.read_csv('originals/new-EventsAdapt-sentences.ids.txt', sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ev=pd.read_csv('originals/new-EventsAdapt-sentences.ids.txt', sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf92c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtfit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9addadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'ev1': prepare_data(ev1),\n",
    "            'dtfit': prepare_data(dtfit),\n",
    "            'ev2': prepare_data(ev2),\n",
    "            'new-EventsAdapt': prepare_data(new_ev)\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-figure",
   "metadata": {},
   "source": [
    "## Transformer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tokenizations   #   pip install pytokenizations  (https://pypi.org/project/pytokenizations/)\n",
    "import tensorflow as tf  #  TensorFlow 2.0 is required (Python 3.5-3.7, Pip 19.0 or later)\n",
    "\n",
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "from transformers import RobertaTokenizer, TFRobertaForMaskedLM\n",
    "from transformers import XLNetTokenizer, TFXLNetLMHeadModel\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_PREDICTIONS = 15\n",
    "\n",
    "dict_tokenizers = {\"bert-base-cased\": BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "                   \"bert-large-cased\": BertTokenizer.from_pretrained('bert-large-cased'),\n",
    "                   \"roberta-large\": RobertaTokenizer.from_pretrained('roberta-large'),\n",
    "                   \"xlnet-large-cased\":XLNetTokenizer.from_pretrained('xlnet-large-cased'),\n",
    "                   \"gpt2-medium\": GPT2Tokenizer.from_pretrained('gpt2-medium')}\n",
    "\n",
    "\n",
    "dict_mlm_models = {\"bert-base-cased\": TFBertForMaskedLM.from_pretrained('bert-base-cased'),\n",
    "                   \"bert-large-cased\": TFBertForMaskedLM.from_pretrained('bert-large-cased'),\n",
    "                   \"roberta-large\": TFRobertaForMaskedLM.from_pretrained('roberta-large'),\n",
    "                   \"xlnet-large-cased\":TFXLNetLMHeadModel.from_pretrained('xlnet-large-cased'),\n",
    "                   \"gpt2-medium\": TFGPT2LMHeadModel.from_pretrained('gpt2-medium')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel:\n",
    "    \n",
    "    def __init__(self, transf_model):\n",
    "        self.model_name = transf_model\n",
    "        self.tokenizer = dict_tokenizers[transf_model]\n",
    "        self.mlm_model = dict_mlm_models[transf_model]\n",
    "        \n",
    "    def prepare_input(self, sentences, pos_ids):\n",
    "        target_tokens = []\n",
    "        sentences_with_mask = []\n",
    "        dependents_indices = []\n",
    "        #sentences = sentences.reset_index(drop=True)\n",
    "        for i in range(len(sentences)):\n",
    "            sent = sentences[i]\n",
    "            id_dep = pos_ids[i]\n",
    "            s = sent.split(\" \")\n",
    "            #print(s, id_dep)\n",
    "            target_token = sent.split(\" \")[id_dep]\n",
    "            \n",
    "            #  check if target token is in dictionary - otherwise add None to the lists     \n",
    "            # BERT\n",
    "            if self.model_name.startswith(\"bert\"):\n",
    "                if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(target_token)) == \"[UNK]\":\n",
    "                    #target_tokens.append(None)\n",
    "                    target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                else:\n",
    "                    target_tokens.append(target_token)\n",
    "            \n",
    "            # RoBERTa\n",
    "            if self.model_name.startswith(\"roberta\"):\n",
    "                if id_dep == 0:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(target_token)\n",
    "                else:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(\"Ġ\"+target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(\"Ġ\"+target_token)\n",
    "                        \n",
    "            if self.model_name.startswith(\"xlnet\"):\n",
    "                if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(u\"\\u2581\"+target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                    #target_tokens.append(None)\n",
    "                    target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                else:\n",
    "                    target_tokens.append(u\"\\u2581\"+target_token)\n",
    "                    #since in sentencepiece tokenizer this symbol is used for whitespace\n",
    "                        \n",
    "            # GPT-2\n",
    "            if self.model_name.startswith(\"gpt\"):\n",
    "                if id_dep == 0:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(\n",
    "                            self.tokenizer.convert_tokens_to_ids(target_token)) == \"<|endoftext|>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(target_token)\n",
    "                else:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(\n",
    "                            self.tokenizer.convert_tokens_to_ids(\"Ġ\" + target_token)) == \"<|endoftext|>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(\"Ġ\" + target_token)\n",
    "                        \n",
    "            # mask the sentence\n",
    "            list_words = []\n",
    "            for w in range(len(sent.split(\" \"))):\n",
    "                if w != id_dep:\n",
    "                    list_words.append(sent.split(\" \")[w])\n",
    "                else:\n",
    "                    if self.model_name.startswith(\"bert\"):\n",
    "                        list_words.append(\"[MASK]\")\n",
    "                    if self.model_name.startswith((\"roberta\", 'xlnet')):\n",
    "                        list_words.append(\"<mask>\")\n",
    "                    if self.model_name.startswith(\"gpt\"):\n",
    "                        list_words.append(sent.split(\" \")[w])  #  mask is not needed for gpt\n",
    "            masked_sent = \" \".join(list_words)\n",
    "            sentences_with_mask.append(masked_sent)\n",
    "            \n",
    "            model_tokenization = self.tokenizer.tokenize(masked_sent)\n",
    "            #print(model_tokenization)\n",
    "            \n",
    "            if self.model_name.startswith(\"bert\"):\n",
    "                dependent_index = model_tokenization.index(\"[MASK]\") + 1  # take into account token [CLS]\n",
    "            if self.model_name.startswith(\"roberta\"):\n",
    "                dependent_index = model_tokenization.index(\"<mask>\") + 1\n",
    "            if self.model_name.startswith(\"gpt\"):\n",
    "                our_tokenization = masked_sent.split(\" \")\n",
    "                other_tokens_2_model_tokens, model_tokens_2_other_tokens = tokenizations.\\\n",
    "                    get_alignments(our_tokenization, model_tokenization)\n",
    "                dependent_index = other_tokens_2_model_tokens[id_dep][0] + 1\n",
    "            if self.model_name.startswith(\"xlnet\"):\n",
    "                dependent_index = model_tokenization.index(\"<mask>\") \n",
    "                #since xlnet tokenizer does not add cls token at the beginning of the sequence\n",
    "                \n",
    "            dependents_indices.append(dependent_index)\n",
    "            i += 1\n",
    "        return target_tokens, sentences_with_mask, dependents_indices\n",
    "    \n",
    "    def compute_filler_probability(self, list_target_words, list_masked_sentences, \\\n",
    "                                   list_dependents_indexes, unidirectional=False):\n",
    "        \n",
    "        if self.model_name.startswith(\"gpt\"):\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            inputs = self.tokenizer([\"<|endoftext|>\" + sent + \"<|endoftext|>\" for sent in list_masked_sentences],\n",
    "                                    padding=True, return_tensors=\"tf\")\n",
    "            # it is necessary to add a token at the beginning of the sentence\n",
    "        elif self.model_name.startswith(\"xlnet\"):\n",
    "            self.tokenizer.padding_side = \"right\" #since instances of xlnet tokenizer by default apply padding to the left\n",
    "            inputs = self.tokenizer(list_masked_sentences, padding=True, return_tensors=\"tf\") \n",
    "        else:\n",
    "            inputs = self.tokenizer(list_masked_sentences, padding=True, return_tensors=\"tf\")\n",
    "            \n",
    "        if not unidirectional:\n",
    "            probabilities_fillers = []\n",
    "            predicted_fillers = []\n",
    "\n",
    "            #print(\"Executing model for batch...\")\n",
    "            #print()\n",
    "            outputs = self.mlm_model(inputs)[0]\n",
    "            for batch_elem, target_word, dep_index in zip(range(outputs.shape[0]), list_target_words,\n",
    "                                                          list_dependents_indexes):\n",
    "                #if target_word is None:\n",
    "                    #probabilities_fillers.append(None)\n",
    "                    #predicted_fillers.append(None)\n",
    "                if type(target_word) == list: # word is OOV, get its subcomponents probability and average them\n",
    "                    prob_subwords = []\n",
    "                    for target_subword in target_word:\n",
    "                        if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                        if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "                        \n",
    "                        prob_subwords.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_subword)])\n",
    "                    #print(probabilities_fillers, prob_subwords, sum(prob_subwords)/len(prob_subwords))\n",
    "                    probabilities_fillers.append(sum(prob_subwords)/len(prob_subwords))\n",
    "                    #idxs_predictions = (-(np.array(all_probabilities))).argsort()[:N_PREDICTIONS]\n",
    "                    #predictions = self.tokenizer.convert_ids_to_tokens([int(index) for index in idxs_predictions])\n",
    "                        \n",
    "                else:\n",
    "                    if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                    if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "\n",
    "                    probabilities_fillers.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_word)])\n",
    "                    \"\"\"\n",
    "                    idxs_predictions = (-(np.array(all_probabilities))).argsort()[:N_PREDICTIONS]\n",
    "                    predictions = self.tokenizer.convert_ids_to_tokens([int(index) for index in idxs_predictions])\n",
    "                    string_predicted_fillers = \"\"\n",
    "                    for word, index in zip(predictions, idxs_predictions):\n",
    "                        string_predicted_fillers += word.replace(\"Ġ\", \"\")+\"_(\"+str(all_probabilities[index])+\")\"+\";\"\n",
    "                    predicted_fillers.append(string_predicted_fillers)\n",
    "                    \"\"\"\n",
    "            return probabilities_fillers#, predicted_fillers    \n",
    "        \n",
    "        else:    \n",
    "            probabilities_uni_fillers = []\n",
    "            predicted_uni_fillers = []\n",
    "            \n",
    "            new_attention_mask = []\n",
    "            for mask, id, sent in zip(inputs[\"attention_mask\"], list_dependents_indexes, list_masked_sentences):\n",
    "                mask_array = np.array([0 for elem in mask])\n",
    "                for i in range(0, id+1):\n",
    "                    mask_array[i] = 1\n",
    "                new_attention_mask.append(tf.convert_to_tensor(mask_array))\n",
    "            inputs[\"attention_mask\"] = tf.convert_to_tensor(new_attention_mask)\n",
    "            #print(\"Executing model for batch...\")\n",
    "            #print()\n",
    "            outputs = self.mlm_model(inputs)[0]\n",
    "            for batch_elem, target_word, dep_index in zip(range(outputs.shape[0]), list_target_words,\n",
    "                                                          list_dependents_indexes):\n",
    "                #if target_word is None:\n",
    "                #    probabilities_uni_fillers.append(None)\n",
    "                if type(target_word) == list: # word is OOV, get its subcomponents probability and average them\n",
    "                    prob_subwords = []\n",
    "                    for target_subword in target_word:\n",
    "                        if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                        if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "                        \n",
    "                        prob_subwords.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_subword)])\n",
    "                    probabilities_uni_fillers.append(sum(prob_subwords)/len(prob_subwords))\n",
    "                else:\n",
    "                    if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                    if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, 0]).numpy()\n",
    "                    probabilities_uni_fillers.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_word)])\n",
    "        \n",
    "            return probabilities_uni_fillers#, predicted_fillers, probabilities_unidirectional\n",
    "    \n",
    "    \n",
    "    def run_prediction(self, data_sequences, indexes, unilateral, batch_dimension=64):\n",
    "        num_sentences = len(data_sequences)\n",
    "        if num_sentences % batch_dimension == 0:\n",
    "            num_batches = num_sentences // batch_dimension\n",
    "        else:\n",
    "            num_batches = num_sentences // batch_dimension + 1\n",
    "        total_scores = []\n",
    "        total_best_fillers = []\n",
    "        total_uni_scores = []\n",
    "        for batch in range(num_batches):\n",
    "            #print()\n",
    "            #print(\"Processing batch {} of {} . Progress: {} ...\".format(batch + 1, num_batches,\n",
    "            #                                                                  np.round((100 / num_batches) * (batch + 1)\n",
    "            #                                                                           , 2)))\n",
    "            if batch != num_batches - 1:\n",
    "                target_words, masked_sentences, positions_dependents = self.\\\n",
    "                    prepare_input(data_sequences[batch * batch_dimension: (batch + 1) * batch_dimension], indexes)\n",
    "                scores = self.compute_filler_probability(target_words, masked_sentences, \n",
    "                                                                                positions_dependents, unilateral)\n",
    "            else:\n",
    "                target_words, masked_sentences, positions_dependents = self.\\\n",
    "                    prepare_input(data_sequences[batch * batch_dimension:], indexes)\n",
    "                scores = self.compute_filler_probability(target_words, masked_sentences,\n",
    "                                                                       positions_dependents, unilateral)\n",
    "            total_scores.extend(scores)\n",
    "            #total_best_fillers.extend(best_fillers)\n",
    "            #total_uni_scores.extend(uni_scores)\n",
    "            \n",
    "        return total_scores#, total_best_fillers, total_uni_scores\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-saying",
   "metadata": {},
   "source": [
    "## 1. Verb prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(sentences):\n",
    "    model_probs = model.run_prediction(sentences, pos, False, BATCH_SIZE)\n",
    "    log_probs = [math.log(x) for x in model_probs]\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "model = TransformerModel('roberta-large')\n",
    "probs = mask_word(sentences)\n",
    "\n",
    "for i, sent, score in zip(idxs_sent,sentences, probs):\n",
    "    print(i, sent, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-quilt",
   "metadata": {},
   "source": [
    "## 2. Sequential word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-scottish",
   "metadata": {},
   "source": [
    "### 2.1 Pseudo-log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c36a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word_by_word(sentences):\n",
    "    results = []\n",
    "    for sent in sentences:\n",
    "        s = sent.split(' ')\n",
    "        ids = [w_id for w_id in range(0, len(s))]\n",
    "        ss = [sent for i in range(0, len(s))]    \n",
    "        # run model\n",
    "        model_probs = model.run_prediction(ss, ids, False, BATCH_SIZE)\n",
    "\n",
    "        try:\n",
    "            results.append((sent, sum(model_probs)))\n",
    "        except TypeError:\n",
    "            results.append((sent, None))\n",
    "            \n",
    "    log_probs = [math.log(x) for x in results]\n",
    "    return log_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "model = TransformerModel('roberta-large')\n",
    "probs = mask_word_by_word(sentences)\n",
    "\n",
    "for i, sent, score in zip(idxs_sent, sentences, probs):\n",
    "    print(i, sent, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sheet",
   "metadata": {},
   "source": [
    "### 2.2 Left-to-right generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "def mask_word_left2right(sentences):\n",
    "    results = []\n",
    "    for sent in tqdm(sentences):\n",
    "        s = sent.split(' ')\n",
    "        ids = [w_id for w_id in range(0, len(s))]\n",
    "        ss = [sent for i in range(0, len(s))]  \n",
    "        model_probs = model.run_prediction(ss, ids, True, BATCH_SIZE)\n",
    "        #try:\n",
    "        results.append((sent, sum(model_probs)))\n",
    "        #except TypeError:\n",
    "        #    results.append((sent, None))\n",
    "    log_probs = [math.log(x[1]) for x in results]\n",
    "    return log_probs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = 'left2right_res/'\n",
    "model = TransformerModel('bert-large-cased')\n",
    "ids, sents=prepare_data(new_ev)\n",
    "d='new-EventsAdapt'\n",
    "probs = mask_word_left2right(sents)\n",
    "with open(os.path.join(out_folder, d+'.'+'bert-large-cased.l2r.txt'), 'w') as fout:\n",
    "    for i, sent,score in zip(ids,sents,probs):\n",
    "        fout.write('{}\\t{}\\t{}\\n'.format(i, sent,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ae046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_folder = 'left2right_res/'\n",
    "model_name = 'xlnet-large-cased'\n",
    "model = TransformerModel(model_name)\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    ids, sents = datasets[d]\n",
    "    probs = mask_word_left2right(sents)\n",
    "    with open(os.path.join(out_folder, '{}.{}.l2r.txt'.format(d, model_name)), 'w') as fout:\n",
    "        for i, sent,score in zip(ids,sents,probs):\n",
    "            fout.write('{}\\t{}\\t{}\\n'.format(i, sent,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6427c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel('bert-large-cased')\n",
    "f=['The babysitter won the game .']\n",
    "for sent in f:\n",
    "    s = sent.split(' ')\n",
    "    ids = [w_id for w_id in range(0, len(s)-1)]\n",
    "    ss = [sent for i in range(0, len(s)-1)]  \n",
    "    #x=model.prepare_input(ss, ids)\n",
    "    #print(x)\n",
    "    model_probs = model.run_prediction(ss, ids, True, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed905fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print([math.log(x) for x in model_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.tokenize('babysitter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
