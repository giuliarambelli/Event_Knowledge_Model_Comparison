{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9459d08a",
   "metadata": {},
   "source": [
    "# ANN sequential word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e258d77",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giuliarambelli/Event_Knowledge_ANN/blob/master/ANNs_sequential-word-prediction.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-middle",
   "metadata": {},
   "source": [
    "## 1. Load the dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f417482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function load for word2word mask tasks (input files must have the first 2 columns for id and sentence)\n",
    "def prepare_data(df):\n",
    "    ids = []\n",
    "    sents = []\n",
    "    for index, row in df.iterrows():\n",
    "        ids.append(row[0])\n",
    "        if row[1][-1]!='.':\n",
    "            sents.append(row[1]+' .')\n",
    "        else:\n",
    "            sents.append(row[1])\n",
    "    return (ids, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa7c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dtfit=pd.read_csv('datasets/DTFit_vassallo_deps.txt', sep='\\t', header=None)\n",
    "ev1=pd.read_csv('datasets/ev1_deps.txt', sep='\\t',header=None)\n",
    "ev2=pd.read_csv('datasets/ev2_deps.txt', sep='\\t',header=None)\n",
    "new_ev=pd.read_csv('datasets/new-EventsAdapt-sentences.ids.txt', sep='\\t',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69cf92c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The actor won the battle</td>\n",
       "      <td>animate-inanimate</td>\n",
       "      <td>AT</td>\n",
       "      <td>actor:nsubj win:root battle:obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The actor won the award</td>\n",
       "      <td>animate-inanimate</td>\n",
       "      <td>T</td>\n",
       "      <td>actor:nsubj win:root award:obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The anchorman told the parable</td>\n",
       "      <td>animate-inanimate</td>\n",
       "      <td>AT</td>\n",
       "      <td>anchorman:nsubj tell:root parable:obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The anchorman told the news</td>\n",
       "      <td>animate-inanimate</td>\n",
       "      <td>T</td>\n",
       "      <td>anchorman:nsubj tell:root news:obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The animal found the map</td>\n",
       "      <td>animate-inanimate</td>\n",
       "      <td>AT</td>\n",
       "      <td>animal:nsubj find:root map:obj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                               1                  2   3  \\\n",
       "0  0        The actor won the battle  animate-inanimate  AT   \n",
       "1  1         The actor won the award  animate-inanimate   T   \n",
       "2  2  The anchorman told the parable  animate-inanimate  AT   \n",
       "3  3     The anchorman told the news  animate-inanimate   T   \n",
       "4  4        The animal found the map  animate-inanimate  AT   \n",
       "\n",
       "                                        4  \n",
       "0        actor:nsubj win:root battle:obj   \n",
       "1         actor:nsubj win:root award:obj   \n",
       "2  anchorman:nsubj tell:root parable:obj   \n",
       "3     anchorman:nsubj tell:root news:obj   \n",
       "4         animal:nsubj find:root map:obj   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtfit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9addadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load just ids and sentences (for word-by-word preds)\n",
    "datasets = {'ev1': prepare_data(ev1),\n",
    "            'dtfit': prepare_data(dtfit),\n",
    "            'ev2': prepare_data(ev2),\n",
    "            'new-EventsAdapt': prepare_data(new_ev)\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-figure",
   "metadata": {},
   "source": [
    "## 2. Transformer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comic-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import tokenizations   #   pip install pytokenizations  (https://pypi.org/project/pytokenizations/)\n",
    "import tensorflow as tf  #  TensorFlow 2.0 is required (Python 3.5-3.7, Pip 19.0 or later)\n",
    "\n",
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "from transformers import RobertaTokenizer, TFRobertaForMaskedLM\n",
    "from transformers import XLNetTokenizer, TFXLNetLMHeadModel\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30e0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLNetLMHeadModel.\n",
      "\n",
      "All the layers of TFXLNetLMHeadModel were initialized from the model checkpoint at xlnet-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetLMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_PREDICTIONS = 15\n",
    "\n",
    "dict_tokenizers = {\"bert-base-cased\": BertTokenizer.from_pretrained('bert-base-cased'),\n",
    "                   \"bert-large-cased\": BertTokenizer.from_pretrained('bert-large-cased'),\n",
    "                   \"roberta-large\": RobertaTokenizer.from_pretrained('roberta-large'),\n",
    "                   \"xlnet-large-cased\":XLNetTokenizer.from_pretrained('xlnet-large-cased'),\n",
    "                   \"gpt2-medium\": GPT2Tokenizer.from_pretrained('gpt2-medium')}\n",
    "\n",
    "\n",
    "dict_mlm_models = {\"bert-base-cased\": TFBertForMaskedLM.from_pretrained('bert-base-cased'),\n",
    "                   \"bert-large-cased\": TFBertForMaskedLM.from_pretrained('bert-large-cased'),\n",
    "                   \"roberta-large\": TFRobertaForMaskedLM.from_pretrained('roberta-large'),\n",
    "                   \"xlnet-large-cased\":TFXLNetLMHeadModel.from_pretrained('xlnet-large-cased'),\n",
    "                   \"gpt2-medium\": TFGPT2LMHeadModel.from_pretrained('gpt2-medium')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "professional-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel:\n",
    "    \n",
    "    def __init__(self, transf_model):\n",
    "        self.model_name = transf_model\n",
    "        self.tokenizer = dict_tokenizers[transf_model]\n",
    "        self.mlm_model = dict_mlm_models[transf_model]\n",
    "        \n",
    "    def prepare_input(self, sentences, pos_ids):\n",
    "        target_tokens = []\n",
    "        sentences_with_mask = []\n",
    "        dependents_indices = []\n",
    "        #sentences = sentences.reset_index(drop=True)\n",
    "        for i in range(len(sentences)):\n",
    "            sent = sentences[i]\n",
    "            id_dep = pos_ids[i]\n",
    "            s = sent.split(\" \")\n",
    "            #print(s, id_dep)\n",
    "            target_token = sent.split(\" \")[id_dep]\n",
    "            \n",
    "            #  check if target token is in dictionary - otherwise add None to the lists     \n",
    "            # BERT\n",
    "            if self.model_name.startswith(\"bert\"):\n",
    "                if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(target_token)) == \"[UNK]\":\n",
    "                    #target_tokens.append(None)\n",
    "                    target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                else:\n",
    "                    target_tokens.append(target_token)\n",
    "            \n",
    "            # RoBERTa\n",
    "            if self.model_name.startswith(\"roberta\"):\n",
    "                if id_dep == 0:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(target_token)\n",
    "                else:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(\"Ġ\"+target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(\"Ġ\"+target_token)\n",
    "                        \n",
    "            if self.model_name.startswith(\"xlnet\"):\n",
    "                if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(u\"\\u2581\"+target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                    #target_tokens.append(None)\n",
    "                    target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                else:\n",
    "                    target_tokens.append(u\"\\u2581\"+target_token)\n",
    "                    #since in sentencepiece tokenizer this symbol is used for whitespace\n",
    "                        \n",
    "            # GPT-2\n",
    "            if self.model_name.startswith(\"gpt\"):\n",
    "                if id_dep == 0:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(\n",
    "                            self.tokenizer.convert_tokens_to_ids(target_token)) == \"<|endoftext|>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(target_token)\n",
    "                else:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(\n",
    "                            self.tokenizer.convert_tokens_to_ids(\"Ġ\" + target_token)) == \"<|endoftext|>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(\"Ġ\" + target_token)\n",
    "                        \n",
    "            # mask the sentence\n",
    "            list_words = []\n",
    "            for w in range(len(sent.split(\" \"))):\n",
    "                if w != id_dep:\n",
    "                    list_words.append(sent.split(\" \")[w])\n",
    "                else:\n",
    "                    if self.model_name.startswith(\"bert\"):\n",
    "                        list_words.append(\"[MASK]\")\n",
    "                    if self.model_name.startswith((\"roberta\", 'xlnet')):\n",
    "                        list_words.append(\"<mask>\")\n",
    "                    if self.model_name.startswith(\"gpt\"):\n",
    "                        list_words.append(sent.split(\" \")[w])  #  mask is not needed for gpt\n",
    "            masked_sent = \" \".join(list_words)\n",
    "            sentences_with_mask.append(masked_sent)\n",
    "            \n",
    "            model_tokenization = self.tokenizer.tokenize(masked_sent)\n",
    "            #print(model_tokenization)\n",
    "            \n",
    "            if self.model_name.startswith(\"bert\"):\n",
    "                dependent_index = model_tokenization.index(\"[MASK]\") + 1  # take into account token [CLS]\n",
    "            if self.model_name.startswith(\"roberta\"):\n",
    "                dependent_index = model_tokenization.index(\"<mask>\") + 1\n",
    "            if self.model_name.startswith(\"gpt\"):\n",
    "                our_tokenization = masked_sent.split(\" \")\n",
    "                other_tokens_2_model_tokens, model_tokens_2_other_tokens = tokenizations.\\\n",
    "                    get_alignments(our_tokenization, model_tokenization)\n",
    "                dependent_index = other_tokens_2_model_tokens[id_dep][0] + 1\n",
    "            if self.model_name.startswith(\"xlnet\"):\n",
    "                dependent_index = model_tokenization.index(\"<mask>\") \n",
    "                #since xlnet tokenizer does not add cls token at the beginning of the sequence\n",
    "                \n",
    "            dependents_indices.append(dependent_index)\n",
    "            i += 1\n",
    "        return target_tokens, sentences_with_mask, dependents_indices\n",
    "    \n",
    "    def compute_filler_probability(self, list_target_words, list_masked_sentences, \\\n",
    "                                   list_dependents_indexes, unidirectional=False):\n",
    "        \n",
    "        if self.model_name.startswith(\"gpt\"):\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            inputs = self.tokenizer([\"<|endoftext|>\" + sent + \"<|endoftext|>\" for sent in list_masked_sentences],\n",
    "                                    padding=True, return_tensors=\"tf\")\n",
    "            # it is necessary to add a token at the beginning of the sentence\n",
    "        elif self.model_name.startswith(\"xlnet\"):\n",
    "            self.tokenizer.padding_side = \"right\" #since instances of xlnet tokenizer by default apply padding to the left\n",
    "            inputs = self.tokenizer(list_masked_sentences, padding=True, return_tensors=\"tf\") \n",
    "        else:\n",
    "            inputs = self.tokenizer(list_masked_sentences, padding=True, return_tensors=\"tf\")\n",
    "            \n",
    "        if not unidirectional:\n",
    "            probabilities_fillers = []\n",
    "            predicted_fillers = []\n",
    "\n",
    "            #print(\"Executing model for batch...\")\n",
    "            #print()\n",
    "            outputs = self.mlm_model(inputs)[0]\n",
    "            for batch_elem, target_word, dep_index in zip(range(outputs.shape[0]), list_target_words,\n",
    "                                                          list_dependents_indexes):\n",
    "                #if target_word is None:\n",
    "                    #probabilities_fillers.append(None)\n",
    "                    #predicted_fillers.append(None)\n",
    "                if type(target_word) == list: # word is OOV, get its subcomponents probability and average them\n",
    "                    prob_subwords = []\n",
    "                    for target_subword in target_word:\n",
    "                        if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                        if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "                        \n",
    "                        prob_subwords.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_subword)])\n",
    "                    #print(probabilities_fillers, prob_subwords, sum(prob_subwords)/len(prob_subwords))\n",
    "                    probabilities_fillers.append(sum(prob_subwords)/len(prob_subwords))\n",
    "                    #idxs_predictions = (-(np.array(all_probabilities))).argsort()[:N_PREDICTIONS]\n",
    "                    #predictions = self.tokenizer.convert_ids_to_tokens([int(index) for index in idxs_predictions])\n",
    "                        \n",
    "                else:\n",
    "                    if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                    if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "\n",
    "                    probabilities_fillers.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_word)])\n",
    "                    \"\"\"\n",
    "                    idxs_predictions = (-(np.array(all_probabilities))).argsort()[:N_PREDICTIONS]\n",
    "                    predictions = self.tokenizer.convert_ids_to_tokens([int(index) for index in idxs_predictions])\n",
    "                    string_predicted_fillers = \"\"\n",
    "                    for word, index in zip(predictions, idxs_predictions):\n",
    "                        string_predicted_fillers += word.replace(\"Ġ\", \"\")+\"_(\"+str(all_probabilities[index])+\")\"+\";\"\n",
    "                    predicted_fillers.append(string_predicted_fillers)\n",
    "                    \"\"\"\n",
    "            return probabilities_fillers#, predicted_fillers    \n",
    "        \n",
    "        else:    \n",
    "            probabilities_uni_fillers = []\n",
    "            predicted_uni_fillers = []\n",
    "            \n",
    "            new_attention_mask = []\n",
    "            for mask, id, sent in zip(inputs[\"attention_mask\"], list_dependents_indexes, list_masked_sentences):\n",
    "                mask_array = np.array([0 for elem in mask])\n",
    "                for i in range(0, id+1):\n",
    "                    mask_array[i] = 1\n",
    "                new_attention_mask.append(tf.convert_to_tensor(mask_array))\n",
    "            inputs[\"attention_mask\"] = tf.convert_to_tensor(new_attention_mask)\n",
    "            #print(\"Executing model for batch...\")\n",
    "            #print()\n",
    "            outputs = self.mlm_model(inputs)[0]\n",
    "            for batch_elem, target_word, dep_index in zip(range(outputs.shape[0]), list_target_words,\n",
    "                                                          list_dependents_indexes):\n",
    "                #if target_word is None:\n",
    "                #    probabilities_uni_fillers.append(None)\n",
    "                if type(target_word) == list: # word is OOV, get its subcomponents probability and average them\n",
    "                    prob_subwords = []\n",
    "                    for target_subword in target_word:\n",
    "                        if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                        if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "                        \n",
    "                        prob_subwords.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_subword)])\n",
    "                    probabilities_uni_fillers.append(sum(prob_subwords)/len(prob_subwords))\n",
    "                else:\n",
    "                    if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                    if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, 0]).numpy()\n",
    "                    probabilities_uni_fillers.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_word)])\n",
    "        \n",
    "            return probabilities_uni_fillers#, predicted_fillers, probabilities_unidirectional\n",
    "    \n",
    "    \n",
    "    def run_prediction(self, data_sequences, indexes, unilateral, batch_dimension=64):\n",
    "        num_sentences = len(data_sequences)\n",
    "        if num_sentences % batch_dimension == 0:\n",
    "            num_batches = num_sentences // batch_dimension\n",
    "        else:\n",
    "            num_batches = num_sentences // batch_dimension + 1\n",
    "        total_scores = []\n",
    "        total_best_fillers = []\n",
    "        total_uni_scores = []\n",
    "        for batch in range(num_batches):\n",
    "            #print()\n",
    "            #print(\"Processing batch {} of {} . Progress: {} ...\".format(batch + 1, num_batches,\n",
    "            #                                                                  np.round((100 / num_batches) * (batch + 1)\n",
    "            #                                                                           , 2)))\n",
    "            if batch != num_batches - 1:\n",
    "                target_words, masked_sentences, positions_dependents = self.\\\n",
    "                    prepare_input(data_sequences[batch * batch_dimension: (batch + 1) * batch_dimension], indexes)\n",
    "                scores = self.compute_filler_probability(target_words, masked_sentences, \n",
    "                                                                                positions_dependents, unilateral)\n",
    "            else:\n",
    "                target_words, masked_sentences, positions_dependents = self.\\\n",
    "                    prepare_input(data_sequences[batch * batch_dimension:], indexes)\n",
    "                scores = self.compute_filler_probability(target_words, masked_sentences,\n",
    "                                                                       positions_dependents, unilateral)\n",
    "            total_scores.extend(scores)\n",
    "            #total_best_fillers.extend(best_fillers)\n",
    "            #total_uni_scores.extend(uni_scores)\n",
    "            \n",
    "        return total_scores#, total_best_fillers, total_uni_scores\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-quilt",
   "metadata": {},
   "source": [
    "## 3. TASK: Sequential word prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-scottish",
   "metadata": {},
   "source": [
    "### 3.1 Pseudo-log likelihood\n",
    "Given a sentence, mask each word and compute its probability (log-transformed). \n",
    "The final score of the sentence is the sum of words probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c36a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word_by_word(sentences):\n",
    "    results = []\n",
    "    for sent in sentences:\n",
    "        s = sent.split(' ')\n",
    "        ids = [w_id for w_id in range(0, len(s))]\n",
    "        ss = [sent for i in range(0, len(s))]    \n",
    "        # run model\n",
    "        model_probs = model.run_prediction(ss, ids, False, BATCH_SIZE)\n",
    "\n",
    "        try:\n",
    "            results.append((sent, sum(model_probs)))\n",
    "        except TypeError:\n",
    "            results.append((sent, None))\n",
    "            \n",
    "    log_probs = [math.log(x) for x in results]\n",
    "    return log_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00494ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN MODEL\n",
    "\n",
    "model_name = 'roberta-large'\n",
    "out_dir = 'word_by_word/'\n",
    "out_dir = os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "model = TransformerModel(model_name)\n",
    "\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    ids, sents = datasets[d]\n",
    "    probs = mask_word_by_word(sents)\n",
    "    with open(os.path.join(out_folder, '{}.{}.l2r.txt'.format(d, model_name)), 'w') as fout:\n",
    "        for i, sent,score in zip(ids,sents,probs):\n",
    "            fout.write('{}\\t{}\\t{}\\n'.format(i, sent,score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-sheet",
   "metadata": {},
   "source": [
    "### 2.2 Left-to-right generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "def mask_word_left2right(sentences):\n",
    "    results = []\n",
    "    for sent in tqdm(sentences):\n",
    "        s = sent.split(' ')\n",
    "        ids = [w_id for w_id in range(0, len(s))]\n",
    "        ss = [sent for i in range(0, len(s))]  \n",
    "        model_probs = model.run_prediction(ss, ids, True, BATCH_SIZE)\n",
    "\n",
    "        results.append((sent, sum(model_probs)))\n",
    "\n",
    "    log_probs = [math.log(x[1]) for x in results]\n",
    "    return log_probs\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ae046",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = 'left2right_res/'\n",
    "model_name = 'xlnet-large-cased'\n",
    "out_dir = os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "model = TransformerModel(model_name)\n",
    "\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    ids, sents = datasets[d]\n",
    "    probs = mask_word_left2right(sents)\n",
    "    with open(os.path.join(out_folder, '{}.{}.l2r.txt'.format(d, model_name)), 'w') as fout:\n",
    "        for i, sent,score in zip(ids,sents,probs):\n",
    "            fout.write('{}\\t{}\\t{}\\n'.format(i, sent,score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
