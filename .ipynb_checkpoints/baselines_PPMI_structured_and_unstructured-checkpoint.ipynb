{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daily-debut",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "Compute baseline structured and unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125caa9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giuliarambelli/Event_Knowledge_Model_Comparison/blob/master/baselines_PPMI_structured_and_unstructured.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chief-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unlike-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def laplace(ab,a,b,n,v):\n",
    "\texp = np.true_divide(((a+1) * (b+1)), n+v)\n",
    "\tresult = np.log2(np.true_divide(ab+2, exp))\n",
    "\treturn max(0, result)\n",
    "\n",
    "\n",
    "def mi(ab, a, b, n):\n",
    "\t'''\n",
    "\tPointwise Mutual Information (Church & Hanks, 1990)\n",
    "\tif returns 'Inf' in case of division by zero\n",
    "\t'''\n",
    "\texp = np.true_divide(a * b, n)\n",
    "\tresult = np.log2(np.true_divide(ab, exp))\n",
    "\treturn result\n",
    "\n",
    "def ppmi(ab, a, b, n):\n",
    "\tres = max(0, mi(ab, a, b, n))\n",
    "\treturn res\n",
    "\n",
    "synrel_dic = {'NSUBJ':'nsubj', 'OBJ':'dobj', 'OBL':'nmod'}\n",
    "\n",
    "def load_formatted(path, b):\n",
    "\td = {}\n",
    "\twith open(path, 'r') as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tline = line.strip()\n",
    "\t\t\tid = int(line.split('\\t')[0])\n",
    "\t\t\titems = line.split('\\t')[1:]\n",
    "\n",
    "\t\t\tif b == 1:\n",
    "\t\t\t\titems = [tuple(i.split('@')) for i in items] #{0: [(actor,N,NSUBJ),(win,V,ROOT),(battle,N,OBJ)]}\n",
    "\t\t\t\td[id] = items\n",
    "\t\t\telse:\n",
    "\t\t\t\titems = [i.split('@')[0] for i in items] #{0: [actor,win,battle)]}\n",
    "\t\t\t\td[id] = items\n",
    "\t\treturn d\n",
    "\n",
    "def load_events(path, e_freq=20):\n",
    "\tprint('Load events from: {}'.format(path))\n",
    "\te_dict = {}\n",
    "\twith gzip.open(path, 'rt') as fin:\n",
    "\t\tfor line in fin:\n",
    "\t\t\titem, freq = line.strip().split('\\t')\n",
    "\t\t\t#if float(freq)> e_freq:\n",
    "\t\t\te_dict[tuple(item.split(' '))] = float(freq)\n",
    "\treturn e_dict\n",
    "\n",
    "def load_events2(path, lemmas, words):\n",
    "\tprint('Load events from: {}'.format(path))\n",
    "\te_dict = {}\n",
    "\twith gzip.open(path, 'rt') as fin:\n",
    "\t\tfor line in fin:\n",
    "\t\t\titem, freq = line.strip().split('\\t')\n",
    "\t\t\tif all( w in lemmas for w in item.split(' ')) and all( j in words for j in item.split(' ')):\n",
    "\t\t\t\te_dict[tuple(item.split(' '))] = float(freq)\n",
    "\treturn e_dict\n",
    "\n",
    "\n",
    "def load_lemmas(lemmas_freqs_file):\n",
    "\tlem_freq_dict = collections.defaultdict(int)\n",
    "\tn = 0\n",
    "\twith gzip.open(lemmas_freqs_file, \"rt\") as fin:\n",
    "\t\tfor line in fin:\n",
    "\t\t\tword, freq = line.strip().split('\\t')\n",
    "\t\t\tif ' ' in word:\n",
    "\t\t\t\tword = tuple(word.split(' '))\n",
    "\t\t\tlem_freq_dict[word] += float(freq)\n",
    "\t\t\tn+= float(freq)\n",
    "\treturn lem_freq_dict,n\n",
    "\n",
    "\n",
    "def check_coverage(words, all_lemmas):\n",
    "\tfor w in words:\n",
    "\t\tif w not in all_lemmas:\n",
    "\t\t\tprint(w)\n",
    "\n",
    "def events_bigram(events_dict):\n",
    "\tpairs_dict = collections.defaultdict(float)\n",
    "\tword_rel_dic = collections.defaultdict(float)\n",
    "\tn = 0\n",
    "\tfor e in events_dict:\n",
    "\t\tv,a,synrel = e\n",
    "\t\tif synrel.startswith('nsubj'):\n",
    "\t\t\tsynrel = 'nsubj'\n",
    "\t\t#if tuple(v.split('@')) in accepted_lemmas and tuple(a.split('@')) in accepted_lemmas:\n",
    "\t\tword_rel_dic[(v,synrel)]+=events_dict[e]\n",
    "\t\tword_rel_dic[(a, synrel)] += events_dict[e]\n",
    "\t\tpairs_dict[e] = events_dict[e]\n",
    "\t\tn+= events_dict[e]\n",
    "\treturn pairs_dict, word_rel_dic, n\n",
    "\n",
    "\n",
    "def get_lemma(w, dict):\n",
    "\ttry:\n",
    "\t\tl = dict[w]\n",
    "\texcept KeyError:\n",
    "\t\tl = 0\n",
    "\treturn l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-liberty",
   "metadata": {},
   "source": [
    "## 1. Baseline 1\n",
    "**PPMI (structured input, input annotated with grammatical roles)**\n",
    "\n",
    "The score of a sentence is the sum of the PPMIs of syntactic relations <head, dependent, role>\n",
    "Frequencies from ukwac+wiki2018 corpora (f min = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "identified-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline1(data_files, events_file, out_dir, smooth=False):\n",
    "\tevents = load_events(events_file)\n",
    "\t#lemmas = load_lemmas(lemmas_freqs_file)\n",
    "\tevents, wrel, N = events_bigram(events)\n",
    "\n",
    "\tfor data_file in data_files:\n",
    "\t\tprint('Reading:', data_file)\n",
    "\t\tdata = load_formatted(data_file, 1)\n",
    "\n",
    "\t\tres = {}\n",
    "\t\tfor id, item in data.items():\n",
    "\t\t\titem = [(i[0]+'@'+i[1],i[2]) for i in item]\n",
    "\t\t\tppmis = []\n",
    "\t\t\tv = item[1][0]\n",
    "\n",
    "\t\t\trel = 'nsubj'\n",
    "\t\t\ts = item[0][0]\n",
    "\t\t\tif (v, s, rel) not in events:\n",
    "\t\t\t\tsv_freq = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tsv_freq = events[(v, s, rel)]\n",
    "\t\t\tif smooth:\n",
    "\t\t\t\tsv_ppmi = laplace(sv_freq,get_lemma((s, rel), wrel),get_lemma((v, rel), wrel), N, len(events))\n",
    "\t\t\telse:\n",
    "\t\t\t\tsv_ppmi = ppmi(sv_freq,get_lemma((s, rel), wrel),get_lemma((v, rel), wrel), N)\n",
    "\n",
    "\n",
    "\t\t\tppmis.append(sv_ppmi)\n",
    "\n",
    "\t\t\tfor arg in item[2:]:\n",
    "\t\t\t\targ, rel = arg\n",
    "\t\t\t\tif rel == 'OBJ':\n",
    "\t\t\t\t\trel = 'dobj'\n",
    "\t\t\t\telif rel.startswith('OBL'):\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tprep = rel.split(':')[1]\n",
    "\t\t\t\t\t\trel = 'nmod:' + prep\n",
    "\t\t\t\t\texcept IndexError:\n",
    "\t\t\t\t\t\trel = 'nmod:by'\n",
    "\t\t\t\tif (v, arg, rel) not in events:\n",
    "\t\t\t\t\tva_freq = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tva_freq = events[(v, arg, rel)]\n",
    "\t\t\t\tif smooth:\n",
    "\t\t\t\t\tva_ppmi = laplace(va_freq, get_lemma((v, rel), wrel),get_lemma((arg, rel), wrel), N, len(events))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tva_ppmi = ppmi(va_freq, get_lemma((v, rel), wrel),get_lemma((arg, rel), wrel), N)\n",
    "\n",
    "\t\t\t\tppmis.append(va_ppmi)\n",
    "\t\t\tres[id] = ppmis\n",
    "\n",
    "\t\tfname = os.path.basename(data_file).split('.')[0]\n",
    "\t\tdata_sent = pd.read_csv(os.path.join('datasets', fname+'.txt'), sep='\\t', header=None)\n",
    "\t\twith open(os.path.join(out_dir, fname+'.scores_baseline1.txt'), 'w') as fout:\n",
    "\t\t\tfor id in sorted(data):\n",
    "\t\t\t\tsent = data_sent.iloc[id][1]\n",
    "\t\t\t\tprint('{}\\t{}\\t{}'.format(id, sent, sum(res[id])), file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-bedroom",
   "metadata": {},
   "source": [
    "## Baseline 2\n",
    "**ngram sentence surprisal**\n",
    "\n",
    "The score of a sentence is the sum of the PPMIs of each bigram in the sentence.\n",
    "Frequencies from ukwac+wiki2018 corpora (f min = 5). Bigrams are considered in a window +-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expensive-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline2(data_files, events_file, lemmas_freqs_file, out_dir, smooth=False):\n",
    "\tlemmas_freq, N = load_lemmas(lemmas_freqs_file)\n",
    "\t#events = load_events(events_file)\n",
    "\t#events = load_events2(events_file, lemmas_freq.keys())\n",
    "\tfor data_file in data_files:\n",
    "\t\tprint('Reading:', data_file)\n",
    "\t\tdata = load_formatted(data_file, 2)\n",
    "\t\twords = set(itertools.chain(*data.values()))\n",
    "\t\tevents = load_events2(events_file, lemmas_freq.keys(), words)\n",
    "\t\tres = {}\n",
    "\t\tfor id, item in data.items():\n",
    "\t\t\tppmis = []\n",
    "\t\t\tif (item[0], item[1]) not in events:\n",
    "\t\t\t\tsv_freq = 0\n",
    "\t\t\telse:\n",
    "\t\t\t\tsv_freq = events[(item[0], item[1])]\n",
    "\t\t\tif smooth:\n",
    "\t\t\t\tsv_ppmi = laplace(sv_freq, get_lemma(item[0], lemmas_freq), get_lemma(item[1], lemmas_freq), N, len(lemmas_freq))\n",
    "\t\t\telse:\n",
    "\t\t\t\tsv_ppmi = ppmi(sv_freq, get_lemma(item[0], lemmas_freq), get_lemma(item[1], lemmas_freq), N)\n",
    "\n",
    "\t\t\tppmis.append(sv_ppmi)\n",
    "\n",
    "\t\t\tfor arg in item[2:]:\n",
    "\t\t\t\tif (item[1], arg) not in events:\n",
    "\t\t\t\t\tva_freq = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tva_freq = events[(item[1], arg)]\n",
    "\t\t\t\tif smooth:\n",
    "\t\t\t\t\tva_ppmi = laplace(va_freq, get_lemma(item[1], lemmas_freq), get_lemma(arg, lemmas_freq), N, len(lemmas_freq))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tva_ppmi = ppmi(va_freq, get_lemma(item[1], lemmas_freq), get_lemma(arg, lemmas_freq), N)\n",
    "\n",
    "\t\t\t\tppmis.append(va_ppmi)\n",
    "\t\t\tres[id] = ppmis\n",
    "\n",
    "\t\tfname = os.path.basename(data_file).split('.')[0]\n",
    "\t\tdata_sent = pd.read_csv(os.path.join('datasets', fname+'.txt'), sep='\\t', header=None)\n",
    "\t\twith open(os.path.join(out_dir, fname+'.scores_baseline2.txt'), 'w') as fout:\n",
    "\t\t\tfor id in sorted(data):\n",
    "\t\t\t\tsent = data_sent.iloc[id][1]\n",
    "\t\t\t\tprint('{}\\t{}'.format(id, sum(res[id])), file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-converter",
   "metadata": {},
   "source": [
    "### Run script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fifteen-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "smooth = True # apply laplace or not\n",
    "\n",
    "#list dataset paths (specific format) \n",
    "f= ['datasets/parsed/new-EventsAdapt-sentences.txt']\n",
    "\n",
    "out_dir = 'baseline_res/smoothed/'\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "earned-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load events from: freqs/events_baseline1-freqs.2.filtered.gz\n",
      "Reading: datasets/parsed/new-EventsAdapt-sentences.txt\n"
     ]
    }
   ],
   "source": [
    "# baseline 1\n",
    "#lem_path = 'freqs/lempos-freqs.50.filtered.gz'\n",
    "event_path = 'freqs/events_baseline1-freqs.2.filtered.gz'\n",
    "baseline1(f, event_path, out_dir, smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "experienced-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: datasets/parsed/new-EventsAdapt-sentences.txt\n",
      "Load events from: freqs/events_baseline2-freqs.5.filtered.gz\n"
     ]
    }
   ],
   "source": [
    "# baseline 2\n",
    "lempath = 'freqs/lemma-freqs.50.filtered.gz'\n",
    "event_path = 'freqs/events_baseline2-freqs.5.filtered.gz'\n",
    "baseline2(f, event_path, lempath, out_dir, smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b09d53",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Lemma and Event frequency files are not in github directory for space reason. Please contact the authors for getting these files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
