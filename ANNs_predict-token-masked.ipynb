{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a1dbf49",
   "metadata": {},
   "source": [
    "# Token prediction (bidirectional ANNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6f5d0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giuliarambelli//Event_Knowledge_Model_Comparison/blob/master/ANNs_predict-token-masked.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a04628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import os\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-middle",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intelligent-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function load the datasets modified, the 3rd column is the position of the word we have to mask\n",
    "def load_data(df):\n",
    "    ids = []\n",
    "    sents = []\n",
    "    pos = []\n",
    "    for index, row in df.iterrows():\n",
    "        ids.append(row[0])\n",
    "        pos.append(row[2]) \n",
    "        if row[1][-1]!='.':\n",
    "            sents.append(row[1]+' .')\n",
    "        else:\n",
    "            sents.append(row[1])\n",
    "    #return (ids, sents)\n",
    "    return ids, sents, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85574851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to files in dataset/id_verbs subdirectory (position of the verb has to be given)\n",
    "dtfit=pd.read_csv('datasets/id_verbs/DTFit_vassallo_deps.verbs.txt', sep='\\t', header=None)\n",
    "ev1=pd.read_csv('datasets/id_verbs/ev1_deps.verbs.txt', sep='\\t', header=None)\n",
    "events_adapt=pd.read_csv('datasets/id_verbs/newsentences_EventsAdapt.verbs.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cf92c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The raider caught the illness .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The illness caught the raider .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The illness was caught by the raider .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The raider was caught by the illness .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The marauder contracted the disease .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                       1  2\n",
       "0  0         The raider caught the illness .  2\n",
       "1  1         The illness caught the raider .  2\n",
       "2  2  The illness was caught by the raider .  3\n",
       "3  3  The raider was caught by the illness .  3\n",
       "4  4   The marauder contracted the disease .  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_adapt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9addadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'ev1': load_data(ev1),\n",
    "            'dtfit': load_data(dtfit),\n",
    "            'new-EventsAdapt': load_data(events_adapt)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comic-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install pytokenizations\n",
    "#!pip install sentencepiece\n",
    "import numpy as np\n",
    "import tokenizations   #   pip install pytokenizations  (https://pypi.org/project/pytokenizations/)\n",
    "import tensorflow as tf  #  TensorFlow 2.0 is required (Python 3.5-3.7, Pip 19.0 or later)\n",
    "\n",
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "from transformers import RobertaTokenizer, TFRobertaForMaskedLM\n",
    "from transformers import XLNetTokenizer, TFXLNetLMHeadModel\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30e0ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaForMaskedLM.\n",
      "\n",
      "All the layers of TFRobertaForMaskedLM were initialized from the model checkpoint at roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFXLNetLMHeadModel.\n",
      "\n",
      "All the layers of TFXLNetLMHeadModel were initialized from the model checkpoint at xlnet-large-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_PREDICTIONS = 15\n",
    "\n",
    "dict_tokenizers = {\"bert-large-cased\": BertTokenizer.from_pretrained('bert-large-cased'),\n",
    "                   \"roberta-large\": RobertaTokenizer.from_pretrained('roberta-large'),\n",
    "                   \"xlnet-large-cased\":XLNetTokenizer.from_pretrained('xlnet-large-cased')}\n",
    "\n",
    "\n",
    "dict_mlm_models = {\"bert-large-cased\": TFBertForMaskedLM.from_pretrained('bert-large-cased'),\n",
    "                   \"roberta-large\": TFRobertaForMaskedLM.from_pretrained('roberta-large'),\n",
    "                   \"xlnet-large-cased\":TFXLNetLMHeadModel.from_pretrained('xlnet-large-cased')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "professional-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel:\n",
    "    \n",
    "    def __init__(self, transf_model):\n",
    "        self.model_name = transf_model\n",
    "        self.tokenizer = dict_tokenizers[transf_model]\n",
    "        self.mlm_model = dict_mlm_models[transf_model]\n",
    "        \n",
    "    def prepare_input(self, sentences, pos_ids):\n",
    "        target_tokens = []\n",
    "        sentences_with_mask = []\n",
    "        dependents_indices = []\n",
    "        #sentences = sentences.reset_index(drop=True)\n",
    "        for i in range(len(sentences)):\n",
    "            sent = sentences[i]\n",
    "            id_dep = pos_ids[i]\n",
    "            s = sent.split(\" \")\n",
    "            #print(s, id_dep)\n",
    "            target_token = sent.split(\" \")[id_dep]\n",
    "            \n",
    "            #  check if target token is in dictionary - otherwise add None to the lists     \n",
    "            # BERT\n",
    "            if self.model_name.startswith(\"bert\"):\n",
    "                if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(target_token)) == \"[UNK]\":\n",
    "                    #target_tokens.append(None)\n",
    "                    target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                else:\n",
    "                    target_tokens.append(target_token)\n",
    "            \n",
    "            # RoBERTa\n",
    "            if self.model_name.startswith(\"roberta\"):\n",
    "                if id_dep == 0:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(target_token)\n",
    "                else:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(\"Ġ\"+target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(\"Ġ\"+target_token)\n",
    "                        \n",
    "            if self.model_name.startswith(\"xlnet\"):\n",
    "                if self.tokenizer.convert_ids_to_tokens(self.tokenizer.convert_tokens_to_ids(u\"\\u2581\"+target_token)) == \\\n",
    "                            \"<unk>\":\n",
    "                    #target_tokens.append(None)\n",
    "                    target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                else:\n",
    "                    target_tokens.append(u\"\\u2581\"+target_token)\n",
    "                    #since in sentencepiece tokenizer this symbol is used for whitespace\n",
    "                        \n",
    "            # GPT-2\n",
    "            if self.model_name.startswith(\"gpt\"):\n",
    "                if id_dep == 0:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(\n",
    "                            self.tokenizer.convert_tokens_to_ids(target_token)) == \"<|endoftext|>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(target_token)\n",
    "                else:\n",
    "                    if self.tokenizer.convert_ids_to_tokens(\n",
    "                            self.tokenizer.convert_tokens_to_ids(\"Ġ\" + target_token)) == \"<|endoftext|>\":\n",
    "                        #target_tokens.append(None)\n",
    "                        target_tokens.append(self.tokenizer.tokenize(target_token))\n",
    "                    else:\n",
    "                        target_tokens.append(\"Ġ\" + target_token)\n",
    "                        \n",
    "            # mask the sentence\n",
    "            list_words = []\n",
    "            for w in range(len(sent.split(\" \"))):\n",
    "                if w != id_dep:\n",
    "                    list_words.append(sent.split(\" \")[w])\n",
    "                else:\n",
    "                    if self.model_name.startswith(\"bert\"):\n",
    "                        list_words.append(\"[MASK]\")\n",
    "                    if self.model_name.startswith((\"roberta\", 'xlnet')):\n",
    "                        list_words.append(\"<mask>\")\n",
    "                    if self.model_name.startswith(\"gpt\"):\n",
    "                        list_words.append(sent.split(\" \")[w])  #  mask is not needed for gpt\n",
    "            masked_sent = \" \".join(list_words)\n",
    "            sentences_with_mask.append(masked_sent)\n",
    "            \n",
    "            model_tokenization = self.tokenizer.tokenize(masked_sent)\n",
    "            #print(model_tokenization)\n",
    "            \n",
    "            if self.model_name.startswith(\"bert\"):\n",
    "                dependent_index = model_tokenization.index(\"[MASK]\") + 1  # take into account token [CLS]\n",
    "            if self.model_name.startswith(\"roberta\"):\n",
    "                dependent_index = model_tokenization.index(\"<mask>\") + 1\n",
    "            if self.model_name.startswith(\"gpt\"):\n",
    "                our_tokenization = masked_sent.split(\" \")\n",
    "                other_tokens_2_model_tokens, model_tokens_2_other_tokens = tokenizations.\\\n",
    "                    get_alignments(our_tokenization, model_tokenization)\n",
    "                dependent_index = other_tokens_2_model_tokens[id_dep][0] + 1\n",
    "            if self.model_name.startswith(\"xlnet\"):\n",
    "                dependent_index = model_tokenization.index(\"<mask>\") \n",
    "                #since xlnet tokenizer does not add cls token at the beginning of the sequence\n",
    "                \n",
    "            dependents_indices.append(dependent_index)\n",
    "            i += 1\n",
    "        return target_tokens, sentences_with_mask, dependents_indices\n",
    "    \n",
    "    def compute_filler_probability(self, list_target_words, list_masked_sentences, \\\n",
    "                                   list_dependents_indexes, unidirectional=False):\n",
    "        \n",
    "        if self.model_name.startswith(\"gpt\"):\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            inputs = self.tokenizer([\"<|endoftext|>\" + sent + \"<|endoftext|>\" for sent in list_masked_sentences],\n",
    "                                    padding=True, return_tensors=\"tf\")\n",
    "            # it is necessary to add a token at the beginning of the sentence\n",
    "        elif self.model_name.startswith(\"xlnet\"):\n",
    "            self.tokenizer.padding_side = \"right\" #since instances of xlnet tokenizer by default apply padding to the left\n",
    "            inputs = self.tokenizer(list_masked_sentences, padding=True, return_tensors=\"tf\") \n",
    "        else:\n",
    "            inputs = self.tokenizer(list_masked_sentences, padding=True, return_tensors=\"tf\")\n",
    "            \n",
    "        if not unidirectional:\n",
    "            probabilities_fillers = []\n",
    "            predicted_fillers = []\n",
    "\n",
    "            #print(\"Executing model for batch...\")\n",
    "            #print()\n",
    "            outputs = self.mlm_model(inputs)[0]\n",
    "            for batch_elem, target_word, dep_index in zip(range(outputs.shape[0]), list_target_words,\n",
    "                                                          list_dependents_indexes):\n",
    "                #if target_word is None:\n",
    "                    #probabilities_fillers.append(None)\n",
    "                    #predicted_fillers.append(None)\n",
    "                if type(target_word) == list: # word is OOV, get its subcomponents probability and average them\n",
    "                    prob_subwords = []\n",
    "                    for target_subword in target_word:\n",
    "                        if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                        if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "                        \n",
    "                        prob_subwords.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_subword)])\n",
    "                    #print(probabilities_fillers, prob_subwords, sum(prob_subwords)/len(prob_subwords))\n",
    "                    probabilities_fillers.append(sum(prob_subwords)/len(prob_subwords))\n",
    "                    #idxs_predictions = (-(np.array(all_probabilities))).argsort()[:N_PREDICTIONS]\n",
    "                    #predictions = self.tokenizer.convert_ids_to_tokens([int(index) for index in idxs_predictions])\n",
    "                        \n",
    "                else:\n",
    "                    if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                    if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "\n",
    "                    probabilities_fillers.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_word)])\n",
    "                    \"\"\"\n",
    "                    idxs_predictions = (-(np.array(all_probabilities))).argsort()[:N_PREDICTIONS]\n",
    "                    predictions = self.tokenizer.convert_ids_to_tokens([int(index) for index in idxs_predictions])\n",
    "                    string_predicted_fillers = \"\"\n",
    "                    for word, index in zip(predictions, idxs_predictions):\n",
    "                        string_predicted_fillers += word.replace(\"Ġ\", \"\")+\"_(\"+str(all_probabilities[index])+\")\"+\";\"\n",
    "                    predicted_fillers.append(string_predicted_fillers)\n",
    "                    \"\"\"\n",
    "            return probabilities_fillers#, predicted_fillers    \n",
    "        \n",
    "        else:    \n",
    "            probabilities_uni_fillers = []\n",
    "            predicted_uni_fillers = []\n",
    "            \n",
    "            new_attention_mask = []\n",
    "            for mask, id, sent in zip(inputs[\"attention_mask\"], list_dependents_indexes, list_masked_sentences):\n",
    "                mask_array = np.array([0 for elem in mask])\n",
    "                for i in range(0, id+1):\n",
    "                    mask_array[i] = 1\n",
    "                new_attention_mask.append(tf.convert_to_tensor(mask_array))\n",
    "            inputs[\"attention_mask\"] = tf.convert_to_tensor(new_attention_mask)\n",
    "            #print(\"Executing model for batch...\")\n",
    "            #print()\n",
    "            outputs = self.mlm_model(inputs)[0]\n",
    "            for batch_elem, target_word, dep_index in zip(range(outputs.shape[0]), list_target_words,\n",
    "                                                          list_dependents_indexes):\n",
    "                #if target_word is None:\n",
    "                #    probabilities_uni_fillers.append(None)\n",
    "                if type(target_word) == list: # word is OOV, get its subcomponents probability and average them\n",
    "                    prob_subwords = []\n",
    "                    for target_subword in target_word:\n",
    "                        if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                        if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                            all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index - 1]).numpy()\n",
    "                        \n",
    "                        prob_subwords.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_subword)])\n",
    "                    probabilities_uni_fillers.append(sum(prob_subwords)/len(prob_subwords))\n",
    "                else:\n",
    "                    if (self.model_name.startswith(\"bert\")) or (self.model_name.startswith(\"roberta\")):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, dep_index]).numpy()\n",
    "                    if self.model_name.startswith(\"gpt\") or self.model_name.startswith(\"xlnet\"):\n",
    "                        all_probabilities = tf.nn.softmax(outputs[batch_elem, 0]).numpy()\n",
    "                    probabilities_uni_fillers.append(all_probabilities[self.tokenizer.convert_tokens_to_ids(target_word)])\n",
    "        \n",
    "            return probabilities_uni_fillers#, predicted_fillers, probabilities_unidirectional\n",
    "    \n",
    "    \n",
    "    def run_prediction(self, data_sequences, indexes, unilateral=False, batch_dimension=64):\n",
    "        num_sentences = len(data_sequences)\n",
    "        if num_sentences % batch_dimension == 0:\n",
    "            num_batches = num_sentences // batch_dimension\n",
    "        else:\n",
    "            num_batches = num_sentences // batch_dimension + 1\n",
    "        total_scores = []\n",
    "        total_best_fillers = []\n",
    "        total_uni_scores = []\n",
    "        for batch in range(num_batches):\n",
    "            #print()\n",
    "            #print(\"Processing batch {} of {} . Progress: {} ...\".format(batch + 1, num_batches,\n",
    "            #                                                                  np.round((100 / num_batches) * (batch + 1)\n",
    "            #                                                                           , 2)))\n",
    "            if batch != num_batches - 1:\n",
    "                target_words, masked_sentences, positions_dependents = self.\\\n",
    "                    prepare_input(data_sequences[batch * batch_dimension: (batch + 1) * batch_dimension], \n",
    "                                  indexes[batch * batch_dimension: (batch + 1) * batch_dimension])\n",
    "                scores = self.compute_filler_probability(target_words, masked_sentences, \n",
    "                                                                                positions_dependents, unilateral)\n",
    "            else:\n",
    "                target_words, masked_sentences, positions_dependents = self.\\\n",
    "                    prepare_input(data_sequences[batch * batch_dimension:], \n",
    "                                  indexes[batch * batch_dimension:])\n",
    "                scores = self.compute_filler_probability(target_words, masked_sentences,\n",
    "                                                                       positions_dependents, unilateral)\n",
    "            total_scores.extend(scores)\n",
    "            #total_best_fillers.extend(best_fillers)\n",
    "            #total_uni_scores.extend(uni_scores)\n",
    "            \n",
    "        return total_scores#, total_best_fillers, total_uni_scores\n",
    "        \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0a033",
   "metadata": {},
   "source": [
    "## 3. TASK: masked word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0184189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_word(model, sentences, pos):\n",
    "    model_probs = model.run_prediction(sentences, pos)\n",
    "    log_probs = [math.log(x) for x in model_probs]\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5708f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"bert-large-cased\",\"roberta-large\",\"xlnet-large-cased\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-saying",
   "metadata": {},
   "source": [
    "### 3.1 SUB-TASK: Verb prediction\n",
    "Given a sentence, mask the verb and compute its probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "657ed7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'verb-probs/'\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sharp-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probability of the verb\n",
    "def verb_prob(models, name):\n",
    "    ids, sents, verb_id = datasets[name]\n",
    "    \n",
    "    for m in models:\n",
    "        print('Model ',m)\n",
    "        model = TransformerModel(m)\n",
    "        probs_verb = mask_word(model, sents, verb_id)\n",
    "\n",
    "\n",
    "        out_verbs = os.path.join(out_dir, '{}.{}.verb-prob.txt'.format(name, m))\n",
    "        print('Write ', out_verbs)\n",
    "        with open(out_verbs, 'w') as fout:\n",
    "            for i, sent,score in zip(ids,sents,probs_verb):\n",
    "                fout.write('{}\\t{}\\t{}\\n'.format(i, sent,score))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "783ec165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  bert-large-cased\n",
      "Write  verb-probs/ev1.bert-large-cased.verb-prob.txt\n",
      "Model  roberta-large\n",
      "Write  verb-probs/ev1.roberta-large.verb-prob.txt\n",
      "Model  xlnet-large-cased\n",
      "Write  verb-probs/ev1.xlnet-large-cased.verb-prob.txt\n",
      "Model  bert-large-cased\n",
      "Write  verb-probs/dtfit.bert-large-cased.verb-prob.txt\n",
      "Model  roberta-large\n",
      "Write  verb-probs/dtfit.roberta-large.verb-prob.txt\n",
      "Model  xlnet-large-cased\n",
      "Write  verb-probs/dtfit.xlnet-large-cased.verb-prob.txt\n",
      "Model  bert-large-cased\n",
      "Write  verb-probs/new-EventsAdapt.bert-large-cased.verb-prob.txt\n",
      "Model  roberta-large\n",
      "Write  verb-probs/new-EventsAdapt.roberta-large.verb-prob.txt\n",
      "Model  xlnet-large-cased\n",
      "Write  verb-probs/new-EventsAdapt.xlnet-large-cased.verb-prob.txt\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    print('Processing: ', dataset_name)\n",
    "    verb_prob(models, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650475f",
   "metadata": {},
   "source": [
    "### 3.2 SUB-TASK: Lask word prediction \n",
    "Given a sentence, mask the last token and compute its probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f481c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'lastword-probs/'\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9994f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probability of the last word\n",
    "def last_word_prob(models, name):\n",
    "    ids, sents, verb_id = datasets[name]\n",
    "    for m in models:\n",
    "        print('Model ',m)\n",
    "        model = TransformerModel(m)\n",
    "\n",
    "        pos = [len(s.strip().split(' '))-2 for s in sents]\n",
    "        probs_last_word = mask_word(model, sents, pos)\n",
    "\n",
    "        out_lastw = os.path.join(out_dir, '{}.{}.last-word-prob.txt'.format(name, m))\n",
    "        print('Write ', out_lastw)        \n",
    "        with open(out_lastw, 'w') as fout:\n",
    "            for i, sent,score in zip(ids,sents,probs_last_word):\n",
    "                fout.write('{}\\t{}\\t{}\\n'.format(i, sent,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76bd3009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  ev1\n",
      "Model  bert-large-cased\n",
      "Write  lastword-probs/ev1.bert-large-cased.last-word-prob.txt\n",
      "Model  roberta-large\n",
      "Write  lastword-probs/ev1.roberta-large.last-word-prob.txt\n",
      "Model  xlnet-large-cased\n",
      "Write  lastword-probs/ev1.xlnet-large-cased.last-word-prob.txt\n",
      "Processing:  dtfit\n",
      "Model  bert-large-cased\n",
      "Write  lastword-probs/dtfit.bert-large-cased.last-word-prob.txt\n",
      "Model  roberta-large\n",
      "Write  lastword-probs/dtfit.roberta-large.last-word-prob.txt\n",
      "Model  xlnet-large-cased\n",
      "Write  lastword-probs/dtfit.xlnet-large-cased.last-word-prob.txt\n",
      "Processing:  new-EventsAdapt\n",
      "Model  bert-large-cased\n",
      "Write  lastword-probs/new-EventsAdapt.bert-large-cased.last-word-prob.txt\n",
      "Model  roberta-large\n",
      "Write  lastword-probs/new-EventsAdapt.roberta-large.last-word-prob.txt\n",
      "Model  xlnet-large-cased\n",
      "Write  lastword-probs/new-EventsAdapt.xlnet-large-cased.last-word-prob.txt\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    print('Processing: ', dataset_name)\n",
    "    last_word_prob(models, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
