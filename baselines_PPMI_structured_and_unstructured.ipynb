{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daily-debut",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "Compute baseline structured and unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e125caa9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/giuliarambelli/Event_Knowledge_Model_Comparison/blob/master/baselines_PPMI_structured_and_unstructured.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chief-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unlike-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils functions\n",
    "\n",
    "def laplace(ab, a, b, n, v):\n",
    "    \"\"\"\n",
    "    Laplace smoothing\n",
    "    :param ab: ab frequency\n",
    "    :param a: a frequency\n",
    "    :param b: b frequency\n",
    "    :param n: frequency of all bigrams\n",
    "    :param v: vocabulary dimension\n",
    "    :return: smoothed PPMI\n",
    "    \"\"\"\n",
    "    exp = np.true_divide(((a + 1) * (b + 1)), n + v)\n",
    "    result = np.log2(np.true_divide(ab + 2, exp))\n",
    "    return max(0, result)\n",
    "\n",
    "\n",
    "def mi(ab, a, b, n):\n",
    "    \"\"\"\n",
    "    Pointwise Mutual Information (Church & Hanks, 1990)\n",
    "    if returns 'Inf' in case of division by zero\n",
    "    :param ab: ab frequency\n",
    "    :param a: a frequency\n",
    "    :param b: b frequency\n",
    "    :param n: frequency of all bigrams\n",
    "    :return: Mutual Information\n",
    "    \"\"\"\n",
    "    exp = np.true_divide(a * b, n)\n",
    "    result = np.log2(np.true_divide(ab, exp))\n",
    "    return result\n",
    "\n",
    "\n",
    "def ppmi(ab, a, b, n):\n",
    "    \"\"\"\n",
    "    Positive Pointwise mutual information\n",
    "    :param ab: ab frequency\n",
    "    :param a: a frequency\n",
    "    :param b: b frequency\n",
    "    :param n: frequency of all bigrams\n",
    "    :return: PPMI score\n",
    "    \"\"\"\n",
    "    res = max(0, mi(ab, a, b, n))\n",
    "    return res\n",
    "\n",
    "\n",
    "def load_mapping(fpath):\n",
    "    \"\"\"\n",
    "    Load a space separated files mapping our relations to UD relation(s)\n",
    "    :param fpath: file mapping role name to UD relation labels\n",
    "    :return: {label:[ud_label(s)]} and {ud_label: label} dictionaries\n",
    "    \"\"\"\n",
    "    map = {}\n",
    "    with open(fpath) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip().split()\n",
    "            map[line[0]] = line[1].split(\",\")\n",
    "    inv_map = {}\n",
    "    for k,v in map.items():\n",
    "        for x in v:\n",
    "            inv_map[x] = k\n",
    "    return map, inv_map\n",
    "\n",
    "\n",
    "def load_formatted(path, b):\n",
    "    \"\"\"\n",
    "    Load datafile where sentence is converted into lemma@pos@rel format (just verb and nouns items in sentence)\n",
    "    'The cop is arresting the criminal' -> cop@N@NSUBJ\tarrest@V@ROOT\tcriminal@N@OBJ\n",
    "    :param path: file path\n",
    "    :param b: type of baseline will be used, it influences the dataformat it will be passed in main function\n",
    "    :return: dictionary where keys are sentence's IDs and values are list containing sentence's items\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            id = int(line.split('\\t')[0])\n",
    "            items = line.split('\\t')[1:]\n",
    "\n",
    "            if b == 1:\n",
    "                items = [tuple(i.split('@')) for i in items]  #{0: [(actor,N,NSUBJ),(win,V,ROOT),(battle,N,OBJ)]}\n",
    "                d[id] = items\n",
    "            else:\n",
    "                items = [i.split('@')[0] for i in items]  #{0: [actor,win,battle)]}\n",
    "                d[id] = items\n",
    "        return d\n",
    "\n",
    "\n",
    "def load_events(path, map, e_freq=0):\n",
    "    \"\"\"\n",
    "    Load events for baseline1\n",
    "    :param path: file path\n",
    "    :param map: dictionary mapping UD rel labels into our labels\n",
    "    :param e_freq: minimum frequency required\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('Load events from: {}'.format(path))\n",
    "    e_dict = {}\n",
    "    with gzip.open(path, 'rt') as fin:\n",
    "        for line in fin:\n",
    "            item, freq = line.strip().split('\\t')\n",
    "            if float(freq) >= e_freq:\n",
    "                w1, w2, rel = item.split(' ')\n",
    "                try:\n",
    "                    e_dict[(w1, w2, map[rel])] = float(freq)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "    return e_dict\n",
    "\n",
    "\n",
    "def load_lemmas(lemmas_freqs_file):\n",
    "    \"\"\"\n",
    "    Load lemmas for baseline2\n",
    "    :param lemmas_freqs_file: file path\n",
    "    :return: [word: freq] dictionary and N as the sum of all frequencies\n",
    "    \"\"\"\n",
    "    lem_freq_dict = collections.defaultdict(int)\n",
    "    n = 0\n",
    "    with gzip.open(lemmas_freqs_file, \"rt\") as fin:\n",
    "        for line in fin:\n",
    "            word, freq = line.strip().split('\\t')\n",
    "            if ' ' in word:\n",
    "                word = tuple(word.split(' '))\n",
    "            lem_freq_dict[word] += float(freq)\n",
    "            n += float(freq)\n",
    "    return lem_freq_dict, n\n",
    "\n",
    "\n",
    "def load_events2(path, lemmas, words):\n",
    "    \"\"\"\n",
    "    Load events for baseline2\n",
    "    :param path: file path\n",
    "    :param lemmas: list of lemmas\n",
    "    :param words:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('Load events from: {}'.format(path))\n",
    "    e_dict = {}\n",
    "    with gzip.open(path, 'rt') as fin:\n",
    "        for line in fin:\n",
    "            item, freq = line.strip().split('\\t')\n",
    "            if all(w in lemmas for w in item.split(' ')) and all(j in words for j in item.split(' ')):\n",
    "                e_dict[tuple(item.split(' '))] = float(freq)\n",
    "    return e_dict\n",
    "\n",
    "\n",
    "def events_bigram(events_dict):\n",
    "    \"\"\"\n",
    "    Compute word-relation frequency from triples\n",
    "    :param events_dict: {((w1, w2, rel): freq} dictionary\n",
    "    :return: {(w1, w2, rel): freq} and {(w, rel): freq} dictionaries\n",
    "    \"\"\"\n",
    "    pairs_dict = collections.defaultdict(float)\n",
    "    word_rel_dic = collections.defaultdict(float)\n",
    "    n = 0\n",
    "    for e in events_dict:\n",
    "        v, a, synrel = e\n",
    "        #if synrel.startswith('nsubj'):\n",
    "        #    synrel = 'nsubj'\n",
    "        #if tuple(v.split('@')) in accepted_lemmas and tuple(a.split('@')) in accepted_lemmas:\n",
    "        word_rel_dic[(v, synrel)] += events_dict[e]\n",
    "        word_rel_dic[(a, synrel)] += events_dict[e]\n",
    "        pairs_dict[e] = events_dict[e]\n",
    "        n += events_dict[e]\n",
    "    return pairs_dict, word_rel_dic, n\n",
    "\n",
    "\n",
    "def get_lemma(w, dict):\n",
    "    try:\n",
    "        l = dict[w]\n",
    "    except KeyError:\n",
    "        l = 0\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-liberty",
   "metadata": {},
   "source": [
    "## 1. Baseline 1\n",
    "**PPMI (structured input, input annotated with grammatical roles)**\n",
    "\n",
    "The score of a sentence is the sum of the PPMIs of syntactic relations <head, dependent, role>\n",
    "Frequencies from ukwac+wiki2018 corpora (f min = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Baseline1(object):\n",
    "    def __init__(self, events_file, map):\n",
    "        events = load_events(events_file, map)\n",
    "        self.events, self.wrel, self.N = events_bigram(events)\n",
    "\n",
    "    def run_baseline(self, data_path, smooth=False, log_path='.'):\n",
    "        data = load_formatted(data_path, 1)\n",
    "        log_out = open('{}/{}.baseline1.log'.format(log_path,os.path.basename(data_path).split('.')[0]),'w')\n",
    "        res = {}\n",
    "\n",
    "        for id, item in data.items():\n",
    "\n",
    "            log_res = []\n",
    "            freq = 0\n",
    "\n",
    "            #print(item)\n",
    "            item = [(i[0] + '@' + i[1], i[2]) for i in item]\n",
    "            ppmis = []\n",
    "            v = item[1][0]\n",
    "\n",
    "            s, rel = item[0]\n",
    "            if (v, s, rel) not in self.events:\n",
    "                sv_freq = 0\n",
    "            else:\n",
    "                sv_freq = self.events[(v, s, rel)]\n",
    "            if smooth:\n",
    "                sv_ppmi = laplace(sv_freq, get_lemma((s, rel), self.wrel), get_lemma((v, rel), self.wrel), self.N,\n",
    "                                  len(self.events))\n",
    "            else:\n",
    "                sv_ppmi = ppmi(sv_freq, get_lemma((s, rel), self.wrel), get_lemma((v, rel), self.wrel), self.N)\n",
    "\n",
    "            log_res.append('{}-{}-{} freq:{} ppmi:{}'.format(v, s, rel, sv_freq, sv_ppmi))\n",
    "            freq += sv_freq\n",
    "            #print(v, s, rel, sv_freq, sv_ppmi)\n",
    "            ppmis.append(sv_ppmi)\n",
    "\n",
    "            for arg in item[2:]:\n",
    "                arg, rel = arg\n",
    "\n",
    "                if (v, arg, rel) not in self.events:\n",
    "                    va_freq = 0\n",
    "                else:\n",
    "                    va_freq = self.events[(v, arg, rel)]\n",
    "                if smooth:\n",
    "                    va_ppmi = laplace(va_freq, get_lemma((v, rel), self.wrel), get_lemma((arg, rel), self.wrel), self.N,\n",
    "                                      len(self.events))\n",
    "                else:\n",
    "                    va_ppmi = ppmi(va_freq, get_lemma((v, rel), self.wrel), get_lemma((arg, rel), self.wrel), self.N)\n",
    "\n",
    "                log_res.append('{}-{}-{} freq:{} ppmi:{}'.format(v, arg, rel, va_freq, va_ppmi))\n",
    "                freq += va_freq\n",
    "                #print(v, arg, rel, va_freq, va_ppmi)\n",
    "                ppmis.append(va_ppmi)\n",
    "            if freq==0:\n",
    "                print(id,'\\t'.join(log_res), 'zero_score', file=log_out)\n",
    "            else:\n",
    "                print(id,'\\t'.join(log_res), 'score', file=log_out)\n",
    "            res[id] = ppmis\n",
    "        return res\n",
    "\n",
    "    def print_result(self, ppmis_dic, original_data, out_dir, print_all=True):\n",
    "        fname = os.path.basename(original_data).split('.')[0]\n",
    "        data_sent = pd.read_csv(original_data, sep='\\t',\n",
    "                                header=None)  #os.path.join('datasets', fname+'.txt'), sep='\\t', header=None)\n",
    "        outpath = os.path.join(out_dir, fname + '.scores_baseline1.txt')\n",
    "        print('Write output: ', outpath)\n",
    "        with open(outpath, 'w') as fout:\n",
    "            for id in data_sent[0]:\n",
    "                sent = data_sent.iloc[id][1]\n",
    "                if print_all:\n",
    "                    print('{}\\t{}\\t{}\\t{}'.format(id, sent, ','.join([str(i) for i in ppmis_dic[id]]), sum(ppmis_dic[id])),file=fout)\n",
    "                else:\n",
    "                    print('{}\\t{}\\t{}'.format(id, sent, sum(ppmis_dic[id])), file=fout)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "requested-bedroom",
   "metadata": {},
   "source": [
    "## Baseline 2\n",
    "**ngram sentence surprisal**\n",
    "\n",
    "The score of a sentence is the sum of the PPMIs of each bigram in the sentence.\n",
    "Frequencies from ukwac+wiki2018 corpora (f min = 5). Bigrams are considered in a window +-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expensive-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline2(data_files, events_file, lemmas_freqs_file, out_dir, smooth=False):\n",
    "    lemmas_freq, N = load_lemmas(lemmas_freqs_file)\n",
    "    #events = load_events(events_file)\n",
    "    #events = load_events2(events_file, lemmas_freq.keys())\n",
    "    for data_file in data_files:\n",
    "        print('Reading:', data_file)\n",
    "        data = load_formatted(data_file, 2)\n",
    "        words = set(itertools.chain(*data.values()))\n",
    "        events = load_events2(events_file, lemmas_freq.keys(), words)\n",
    "        res = {}\n",
    "        for id, item in data.items():\n",
    "            ppmis = []\n",
    "            if (item[0], item[1]) not in events:\n",
    "                sv_freq = 0\n",
    "            else:\n",
    "                sv_freq = events[(item[0], item[1])]\n",
    "            if smooth:\n",
    "                sv_ppmi = laplace(sv_freq, get_lemma(item[0], lemmas_freq), get_lemma(item[1], lemmas_freq), N,\n",
    "                                  len(lemmas_freq))\n",
    "            else:\n",
    "                sv_ppmi = ppmi(sv_freq, get_lemma(item[0], lemmas_freq), get_lemma(item[1], lemmas_freq), N)\n",
    "\n",
    "            ppmis.append(sv_ppmi)\n",
    "\n",
    "            for arg in item[2:]:\n",
    "                if (item[1], arg) not in events:\n",
    "                    va_freq = 0\n",
    "                else:\n",
    "                    va_freq = events[(item[1], arg)]\n",
    "                if smooth:\n",
    "                    va_ppmi = laplace(va_freq, get_lemma(item[1], lemmas_freq), get_lemma(arg, lemmas_freq), N,\n",
    "                                      len(lemmas_freq))\n",
    "                else:\n",
    "                    va_ppmi = ppmi(va_freq, get_lemma(item[1], lemmas_freq), get_lemma(arg, lemmas_freq), N)\n",
    "\n",
    "                ppmis.append(va_ppmi)\n",
    "            res[id] = ppmis\n",
    "\n",
    "        fname = os.path.basename(data_file).split('.')[0]\n",
    "        data_sent = pd.read_csv(os.path.join('datasets', fname + '.txt'), sep='\\t', header=None)\n",
    "        with open(os.path.join(out_dir, fname + '.scores_baseline2.txt'), 'w') as fout:\n",
    "            for id in sorted(data):\n",
    "                sent = data_sent.iloc[id][1]\n",
    "                print('{}\\t{}'.format(id, sum(res[id])), file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-converter",
   "metadata": {},
   "source": [
    "## Run script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. Baseline 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsentences_EventsAdapt.txt\n",
      "Load events from: events_baseline1-freqs.2.filtered.gz\n",
      "Write output:  results/baseline1_smoothed/newsentences_EventsAdapt.scores_baseline1.txt\n",
      "ev1_deps.txt\n",
      "Load events from: events_baseline1-freqs.2.filtered.gz\n",
      "Write output:  results/baseline1_smoothed/ev1_deps.scores_baseline1.txt\n",
      "DTFit_vassallo_deps.txt\n",
      "Load events from: events_baseline1-freqs.2.filtered.gz\n",
      "Write output:  results/baseline1_smoothed/DTFit_vassallo_deps.scores_baseline1.txt\n"
     ]
    }
   ],
   "source": [
    "# baseline 1\n",
    "#lem_path = 'lempos-freqs.50.filtered.gz'\n",
    "event_path = 'events_baseline1-freqs.2.filtered.gz'  # change path according to where the file is stored\n",
    "\n",
    "# Define parameters\n",
    "smooth = True  # apply laplace or not\n",
    "out_dir = 'results/baseline1_smoothed/'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "files = [f for f in os.listdir('datasets/baseline_format/')]\n",
    "for f in files:\n",
    "    print(f)\n",
    "    #load dictionaries to map UD labels into our labels\n",
    "    _, ud2lab = load_mapping(f.split('.')[0]+'.roles_mapping.txt')\n",
    "    b1_model = Baseline1(event_path,ud2lab)\n",
    "    res_dic = b1_model.run_baseline('datasets/baseline_format/'+f, smooth, out_dir)\n",
    "    b1_model.print_result(res_dic,'datasets/'+f, out_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline 2\n",
    "lempath = 'lemma-freqs.50.filtered.gz'  # change path according to where the file is stored \n",
    "event_path = 'events_baseline2-freqs.5.filtered.gz'  # change path according to where the file is stored \n",
    "baseline2(f, event_path, lempath, out_dir, smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b09d53",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "Lemma and Event frequency files are not in github directory for space reason. Please contact the authors for getting these files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}