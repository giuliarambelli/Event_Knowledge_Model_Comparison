{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN86PYQCywyGkwJQgvMxx6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliarambelli/Event_Knowledge/blob/master/model-jupiter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeEWRuXENKFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "#from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktE_Ov1vQSon",
        "colab_type": "text"
      },
      "source": [
        "Experimental code for *What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models*, by Allyson Ettinger.\n",
        "> https://github.com/aetting/lm-diagnostics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGJ2_Uz0Q1VZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "050bec7e-d185-44de-be98-d1eb3d864a58"
      },
      "source": [
        "#!pip install pytorch_pretrained_bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.46)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.46->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1K_CXP1NRza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#def load_model(modeldir):\n",
        "def load_model():\n",
        "    print(\"load model\")\n",
        "    # Load pre-trained model tokenizer (vocabulary)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    # Load pre-trained model (weights)\n",
        "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "    model.eval()\n",
        "    model.to('cuda')\n",
        "    return model,tokenizer\n",
        "\n",
        "\n",
        "def prep_input(input_sents, tokenizer,bert=True):\n",
        "    # Modify data for Language Model Task\n",
        "    print(\"prepare input\")\n",
        "    for sent in input_sents:\n",
        "        masked_index = None\n",
        "        text = []\n",
        "        mtok = '[MASK]'\n",
        "        if not bert:\n",
        "            sent = re.sub('\\[MASK\\]','X',sent)\n",
        "            mtok = 'x</w>'\n",
        "        if bert: text.append('[CLS]')\n",
        "        text += sent.strip().split()\n",
        "        if text[-1] != '.': text.append('.')\n",
        "        if bert: text.append('[SEP]')\n",
        "        text = ' '.join(text)\n",
        "        tokenized_text = tokenizer.tokenize(text)\n",
        "        for i,tok in enumerate(tokenized_text):\n",
        "            if tok == mtok: masked_index = i\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        yield tokens_tensor, masked_index,tokenized_text\n",
        "\n",
        "\n",
        "def get_predictions(input_sents,model,tokenizer,k=5,bert=True):\n",
        "    token_preds = []\n",
        "    tok_probs = []\n",
        "    for tokens_tensor, mi,_ in prep_input(input_sents,tokenizer,bert=bert):\n",
        "        tokens_tensor = tokens_tensor.to('cuda')\n",
        "        with torch.no_grad():\n",
        "            predictions = model(tokens_tensor)\n",
        "        predicted_tokens = []\n",
        "        predicted_token_probs = []\n",
        "        if bert:\n",
        "            softpred = torch.softmax(predictions[0,mi],0)\n",
        "        else:\n",
        "            softpred = torch.softmax(predictions[0, mi, :],0)\n",
        "        top_inds = torch.argsort(softpred,descending=True)[:k].cpu().numpy()\n",
        "        top_probs = [softpred[tgt_ind].item() for tgt_ind in top_inds]\n",
        "        top_tok_preds = tokenizer.convert_ids_to_tokens(top_inds)\n",
        "        if not bert:\n",
        "            top_tok_preds = [re.sub('\\<\\/w\\>','',e) for e in top_tok_preds]\n",
        "\n",
        "        token_preds.append(top_tok_preds)\n",
        "        tok_probs.append(top_probs)\n",
        "    return token_preds,tok_probs\n",
        "\n",
        "def get_probabilities(input_sents,tgtlist,model,tokenizer,bert=True):\n",
        "    print(\"get probabilities\")\n",
        "    token_probs = []\n",
        "    for i,(tokens_tensor, mi,_) in enumerate(prep_input(input_sents,tokenizer,bert=bert)):\n",
        "        tokens_tensor = tokens_tensor.to('cuda')\n",
        "        print(mi)\n",
        "        with torch.no_grad():\n",
        "            predictions = model(tokens_tensor)\n",
        "        tgt = tgtlist[i]\n",
        "        if bert:\n",
        "            softpred = torch.softmax(predictions[0,mi],0)\n",
        "        else:\n",
        "            softpred = torch.softmax(predictions[0, mi, :],0)\n",
        "        try:\n",
        "            tgt_ind = tokenizer.convert_tokens_to_ids([tgt])[0]\n",
        "        except:\n",
        "            this_tgt_prob = np.nan\n",
        "        else:\n",
        "            this_tgt_prob = softpred[tgt_ind].item()\n",
        "        token_probs.append(this_tgt_prob)\n",
        "    return token_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTUTuLajd_wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61d3ed39-47d6-4e5d-a01c-984261f65ea9"
      },
      "source": [
        "# load Bert model\n",
        "model, tokenizer=load_model()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zfiG110bo50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input example\n",
        "sentences = [\"The fireman is rescuing the [MASK]\", \"The criminal is arresting the [MASK]\"]\n",
        "targets = [\"grandmother\", \"cop\"]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ALFf48BehJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ffe5cabd-71be-4390-82a3-723bc77620d0"
      },
      "source": [
        "# Print top N predictions\n",
        "print(get_predictions(sentences, model, tokenizer))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prepare input\n",
            "([['girl', 'woman', 'children', 'victim', 'victims'], ['girl', 'suspect', 'woman', 'murderer', 'criminal']], [[0.14221793413162231, 0.08759653568267822, 0.07673992216587067, 0.04699753597378731, 0.03726351633667946], [0.07381103187799454, 0.06507331132888794, 0.03878232091665268, 0.02652042917907238, 0.02608479931950569]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npYnQHqjciMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7dbeadec-5a6a-48a5-e44a-b40b6bd33302"
      },
      "source": [
        "# Print probability of the target word\n",
        "print(get_probabilities(sentences,targets, model, tokenizer))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "get probabilities\n",
            "prepare input\n",
            "7\n",
            "6\n",
            "[0.00013654639769811183, 0.009434310719370842]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}