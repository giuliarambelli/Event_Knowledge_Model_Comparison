{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRJ7ObiYmMIFX7cITOVZg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuliarambelli/Event_Knowledge/blob/master/model-jupiter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeEWRuXENKFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1K_CXP1NRza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(modeldir):\n",
        "    # Load pre-trained model tokenizer (vocabulary)\n",
        "    tokenizer = BertTokenizer.from_pretrained(modeldir)\n",
        "    # Load pre-trained model (weights)\n",
        "    model = BertForMaskedLM.from_pretrained(modeldir)\n",
        "    model.eval()\n",
        "    model.to('cuda')\n",
        "    return model,tokenizer\n",
        "\n",
        "\n",
        "def prep_input(input_sents, tokenizer,bert=True):\n",
        "    for sent in input_sents:\n",
        "        masked_index = None\n",
        "        text = []\n",
        "        mtok = '[MASK]'\n",
        "        if not bert:\n",
        "            sent = re.sub('\\[MASK\\]','X',sent)\n",
        "            mtok = 'x</w>'\n",
        "        if bert: text.append('[CLS]')\n",
        "        text += sent.strip().split()\n",
        "        if text[-1] != '.': text.append('.')\n",
        "        if bert: text.append('[SEP]')\n",
        "        text = ' '.join(text)\n",
        "        tokenized_text = tokenizer.tokenize(text)\n",
        "        for i,tok in enumerate(tokenized_text):\n",
        "            if tok == mtok: masked_index = i\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "        tokens_tensor = torch.tensor([indexed_tokens])\n",
        "        yield tokens_tensor, masked_index,tokenized_text\n",
        "\n",
        "\n",
        "def get_predictions(input_sents,model,tokenizer,k=5,bert=True):\n",
        "    token_preds = []\n",
        "    tok_probs = []\n",
        "    for tokens_tensor, mi,_ in prep_input(input_sents,tokenizer,bert=bert):\n",
        "        tokens_tensor = tokens_tensor.to('cuda')\n",
        "        with torch.no_grad():\n",
        "            predictions = model(tokens_tensor)\n",
        "        predicted_tokens = []\n",
        "        predicted_token_probs = []\n",
        "        if bert:\n",
        "            softpred = torch.softmax(predictions[0,mi],0)\n",
        "        else:\n",
        "            softpred = torch.softmax(predictions[0, mi, :],0)\n",
        "        top_inds = torch.argsort(softpred,descending=True)[:k].cpu().numpy()\n",
        "        top_probs = [softpred[tgt_ind].item() for tgt_ind in top_inds]\n",
        "        top_tok_preds = tokenizer.convert_ids_to_tokens(top_inds)\n",
        "        if not bert:\n",
        "            top_tok_preds = [re.sub('\\<\\/w\\>','',e) for e in top_tok_preds]\n",
        "\n",
        "        token_preds.append(top_tok_preds)\n",
        "        tok_probs.append(top_probs)\n",
        "    return token_preds,tok_probs\n",
        "\n",
        "def get_probabilities(input_sents,tgtlist,model,tokenizer,bert=True):\n",
        "    token_probs = []\n",
        "    for i,(tokens_tensor, mi,_) in enumerate(prep_input(input_sents,tokenizer,bert=bert)):\n",
        "        tokens_tensor = tokens_tensor.to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(tokens_tensor)\n",
        "        tgt = tgtlist[i]\n",
        "        if bert:\n",
        "            softpred = torch.softmax(predictions[0,mi],0)\n",
        "        else:\n",
        "            softpred = torch.softmax(predictions[0, mi, :],0)\n",
        "        try:\n",
        "            tgt_ind = tokenizer.convert_tokens_to_ids([tgt])[0]\n",
        "        except:\n",
        "            this_tgt_prob = np.nan\n",
        "        else:\n",
        "            this_tgt_prob = softpred[tgt_ind].item()\n",
        "        token_probs.append(this_tgt_prob)\n",
        "    return token_probs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}